{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'E:\\\\Case Comp\\\\NEST\\\\Inference\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Study URL</th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Results</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Interventions</th>\n",
       "      <th>...</th>\n",
       "      <th>Study Design</th>\n",
       "      <th>Other IDs</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Primary Completion Date</th>\n",
       "      <th>Completion Date</th>\n",
       "      <th>First Posted</th>\n",
       "      <th>Results First Posted</th>\n",
       "      <th>Last Update Posted</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Study Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NCT03162666</td>\n",
       "      <td>Patient Outcomes Using an Expandable Spacer</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT03162666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This study is a post-market clinical follow-up...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Degenerative Disc Disease</td>\n",
       "      <td>DEVICE: ALTERA</td>\n",
       "      <td>...</td>\n",
       "      <td>Observational Model: |Time Perspective: p</td>\n",
       "      <td>RGC16-001</td>\n",
       "      <td>2/28/17</td>\n",
       "      <td>11-06-20</td>\n",
       "      <td>11-06-20</td>\n",
       "      <td>5/22/17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/23/21</td>\n",
       "      <td>Rush University Medical Center, Chicago, Illin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NCT04312048</td>\n",
       "      <td>the Effect of Isosorbide Mononitrate in Reduci...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT04312048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The aim of the study is to evaluate the effica...</td>\n",
       "      <td>NO</td>\n",
       "      <td>IUD Insertion Pain</td>\n",
       "      <td>DRUG: Isosorbide mononitrate|DRUG: placebo</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>249</td>\n",
       "      <td>4/15/20</td>\n",
       "      <td>10/25/20</td>\n",
       "      <td>11/30/20</td>\n",
       "      <td>3/18/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/20/21</td>\n",
       "      <td>Ahmed Samy, Giza, 11231, Egypt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NCT03144778</td>\n",
       "      <td>Durvalumab With or Without Tremelimumab in Tre...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT03144778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This phase I trial studies how well durvalumab...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Stage II Oropharyngeal Squamous Cell Carcinoma...</td>\n",
       "      <td>BIOLOGICAL: Durvalumab|BIOLOGICAL: Tremelimumab</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>2016-0805|NCI-2018-01199|2016-0805</td>\n",
       "      <td>07-12-17</td>\n",
       "      <td>3/15/21</td>\n",
       "      <td>3/15/21</td>\n",
       "      <td>05-09-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/18/21</td>\n",
       "      <td>M D Anderson Cancer Center, Houston, Texas, 77...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NCT01592721</td>\n",
       "      <td>Radiation and Cetuximab Plus Intratumoral EGFR...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT01592721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The incorporation of novel targeted therapies ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>Squamous Cell Carcinoma|Head and Neck Cancer</td>\n",
       "      <td>BIOLOGICAL: EGFR Antisense DNA</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: NA|Intervention Model: SINGLE_GROU...</td>\n",
       "      <td>CTRC 11-47|HSC20120131H</td>\n",
       "      <td>2013-04</td>\n",
       "      <td>02-01-22</td>\n",
       "      <td>02-01-22</td>\n",
       "      <td>05-07-12</td>\n",
       "      <td>9/21/22</td>\n",
       "      <td>9/21/22</td>\n",
       "      <td>University of Pittsburgh, Pittsburgh, Pennsylv...</td>\n",
       "      <td>Study Protocol and Statistical Analysis Plan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NCT04253613</td>\n",
       "      <td>Laser Biostimulation in Periodontal Treatment</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT04253613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The use of nonsurgical periodontal treatment, ...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Periodontal Inflammation|Periodontal Diseases</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Observational Model: |Time Perspective: p</td>\n",
       "      <td>OMU KAEK2012/49</td>\n",
       "      <td>1/15/13</td>\n",
       "      <td>6/15/14</td>\n",
       "      <td>6/15/14</td>\n",
       "      <td>02-05-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07-06-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0   NCT Number  \\\n",
       "0             0           0  NCT03162666   \n",
       "1             1           1  NCT04312048   \n",
       "2             2           2  NCT03144778   \n",
       "3             3           3  NCT01592721   \n",
       "4             4           4  NCT04253613   \n",
       "\n",
       "                                         Study Title  \\\n",
       "0        Patient Outcomes Using an Expandable Spacer   \n",
       "1  the Effect of Isosorbide Mononitrate in Reduci...   \n",
       "2  Durvalumab With or Without Tremelimumab in Tre...   \n",
       "3  Radiation and Cetuximab Plus Intratumoral EGFR...   \n",
       "4      Laser Biostimulation in Periodontal Treatment   \n",
       "\n",
       "                                      Study URL Acronym  \\\n",
       "0  https://clinicaltrials.gov/study/NCT03162666     NaN   \n",
       "1  https://clinicaltrials.gov/study/NCT04312048     NaN   \n",
       "2  https://clinicaltrials.gov/study/NCT03144778     NaN   \n",
       "3  https://clinicaltrials.gov/study/NCT01592721     NaN   \n",
       "4  https://clinicaltrials.gov/study/NCT04253613     NaN   \n",
       "\n",
       "                                       Brief Summary Study Results  \\\n",
       "0  This study is a post-market clinical follow-up...            NO   \n",
       "1  The aim of the study is to evaluate the effica...            NO   \n",
       "2  This phase I trial studies how well durvalumab...            NO   \n",
       "3  The incorporation of novel targeted therapies ...           YES   \n",
       "4  The use of nonsurgical periodontal treatment, ...            NO   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0                          Degenerative Disc Disease   \n",
       "1                                 IUD Insertion Pain   \n",
       "2  Stage II Oropharyngeal Squamous Cell Carcinoma...   \n",
       "3       Squamous Cell Carcinoma|Head and Neck Cancer   \n",
       "4      Periodontal Inflammation|Periodontal Diseases   \n",
       "\n",
       "                                     Interventions  ...  \\\n",
       "0                                   DEVICE: ALTERA  ...   \n",
       "1       DRUG: Isosorbide mononitrate|DRUG: placebo  ...   \n",
       "2  BIOLOGICAL: Durvalumab|BIOLOGICAL: Tremelimumab  ...   \n",
       "3                   BIOLOGICAL: EGFR Antisense DNA  ...   \n",
       "4                                              NaN  ...   \n",
       "\n",
       "                                        Study Design  \\\n",
       "0          Observational Model: |Time Perspective: p   \n",
       "1  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "2  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "3  Allocation: NA|Intervention Model: SINGLE_GROU...   \n",
       "4          Observational Model: |Time Perspective: p   \n",
       "\n",
       "                            Other IDs Start Date Primary Completion Date  \\\n",
       "0                           RGC16-001    2/28/17                11-06-20   \n",
       "1                                 249    4/15/20                10/25/20   \n",
       "2  2016-0805|NCI-2018-01199|2016-0805   07-12-17                 3/15/21   \n",
       "3             CTRC 11-47|HSC20120131H    2013-04                02-01-22   \n",
       "4                     OMU KAEK2012/49    1/15/13                 6/15/14   \n",
       "\n",
       "  Completion Date First Posted Results First Posted Last Update Posted  \\\n",
       "0        11-06-20      5/22/17                  NaN            8/23/21   \n",
       "1        11/30/20      3/18/20                  NaN            1/20/21   \n",
       "2         3/15/21     05-09-17                  NaN            3/18/21   \n",
       "3        02-01-22     05-07-12              9/21/22            9/21/22   \n",
       "4         6/15/14     02-05-20                  NaN           07-06-22   \n",
       "\n",
       "                                           Locations  \\\n",
       "0  Rush University Medical Center, Chicago, Illin...   \n",
       "1                     Ahmed Samy, Giza, 11231, Egypt   \n",
       "2  M D Anderson Cancer Center, Houston, Texas, 77...   \n",
       "3  University of Pittsburgh, Pittsburgh, Pennsylv...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     Study Documents  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  Study Protocol and Statistical Analysis Plan, ...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parquet_file_path = folder_path + 'Usecase_3_.csv'\n",
    "# usecase_3.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "# Load the Parquet file back into a DataFrame\n",
    "data = pd.read_csv(parquet_file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'NCT Number', 'Study Title', 'Study URL',\n",
       "       'Acronym', 'Brief Summary', 'Study Results', 'Conditions',\n",
       "       'Interventions', 'Primary Outcome Measures',\n",
       "       'Secondary Outcome Measures', 'Other Outcome Measures', 'Sponsor',\n",
       "       'Collaborators', 'Sex', 'Age', 'Phases', 'Enrollment', 'Funder Type',\n",
       "       'Study Type', 'Study Design', 'Other IDs', 'Start Date',\n",
       "       'Primary Completion Date', 'Completion Date', 'First Posted',\n",
       "       'Results First Posted', 'Last Update Posted', 'Locations',\n",
       "       'Study Documents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in data.columns if col.startswith('Unnamed')]\n",
    "data.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Study URL</th>\n",
       "      <th>Acronym</th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Results</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Interventions</th>\n",
       "      <th>Primary Outcome Measures</th>\n",
       "      <th>Secondary Outcome Measures</th>\n",
       "      <th>...</th>\n",
       "      <th>Study Design</th>\n",
       "      <th>Other IDs</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Primary Completion Date</th>\n",
       "      <th>Completion Date</th>\n",
       "      <th>First Posted</th>\n",
       "      <th>Results First Posted</th>\n",
       "      <th>Last Update Posted</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Study Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT03162666</td>\n",
       "      <td>Patient Outcomes Using an Expandable Spacer</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT03162666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This study is a post-market clinical follow-up...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Degenerative Disc Disease</td>\n",
       "      <td>DEVICE: ALTERA</td>\n",
       "      <td>Change in Radiographic Analysis, Global and Se...</td>\n",
       "      <td>Complications, Surgical or device related comp...</td>\n",
       "      <td>...</td>\n",
       "      <td>Observational Model: |Time Perspective: p</td>\n",
       "      <td>RGC16-001</td>\n",
       "      <td>2/28/17</td>\n",
       "      <td>11-06-20</td>\n",
       "      <td>11-06-20</td>\n",
       "      <td>5/22/17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/23/21</td>\n",
       "      <td>Rush University Medical Center, Chicago, Illin...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT04312048</td>\n",
       "      <td>the Effect of Isosorbide Mononitrate in Reduci...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT04312048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The aim of the study is to evaluate the effica...</td>\n",
       "      <td>NO</td>\n",
       "      <td>IUD Insertion Pain</td>\n",
       "      <td>DRUG: Isosorbide mononitrate|DRUG: placebo</td>\n",
       "      <td>pain during IUD insertion, intensity of patien...</td>\n",
       "      <td>duration of IUD insertion, duration of IUD ins...</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>249</td>\n",
       "      <td>4/15/20</td>\n",
       "      <td>10/25/20</td>\n",
       "      <td>11/30/20</td>\n",
       "      <td>3/18/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/20/21</td>\n",
       "      <td>Ahmed Samy, Giza, 11231, Egypt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT03144778</td>\n",
       "      <td>Durvalumab With or Without Tremelimumab in Tre...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT03144778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This phase I trial studies how well durvalumab...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Stage II Oropharyngeal Squamous Cell Carcinoma...</td>\n",
       "      <td>BIOLOGICAL: Durvalumab|BIOLOGICAL: Tremelimumab</td>\n",
       "      <td>Change of CD8+ tumor infiltrating lymphocytes,...</td>\n",
       "      <td>Incidence of adverse events and serious advers...</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: RANDOMIZED|Intervention Model: PAR...</td>\n",
       "      <td>2016-0805|NCI-2018-01199|2016-0805</td>\n",
       "      <td>07-12-17</td>\n",
       "      <td>3/15/21</td>\n",
       "      <td>3/15/21</td>\n",
       "      <td>05-09-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/18/21</td>\n",
       "      <td>M D Anderson Cancer Center, Houston, Texas, 77...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT01592721</td>\n",
       "      <td>Radiation and Cetuximab Plus Intratumoral EGFR...</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT01592721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The incorporation of novel targeted therapies ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>Squamous Cell Carcinoma|Head and Neck Cancer</td>\n",
       "      <td>BIOLOGICAL: EGFR Antisense DNA</td>\n",
       "      <td>Toxicity Rate, This is a 2-stage clinical tria...</td>\n",
       "      <td>Tumor Response, Clinical secondary endpoints i...</td>\n",
       "      <td>...</td>\n",
       "      <td>Allocation: NA|Intervention Model: SINGLE_GROU...</td>\n",
       "      <td>CTRC 11-47|HSC20120131H</td>\n",
       "      <td>2013-04</td>\n",
       "      <td>02-01-22</td>\n",
       "      <td>02-01-22</td>\n",
       "      <td>05-07-12</td>\n",
       "      <td>9/21/22</td>\n",
       "      <td>9/21/22</td>\n",
       "      <td>University of Pittsburgh, Pittsburgh, Pennsylv...</td>\n",
       "      <td>Study Protocol and Statistical Analysis Plan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT04253613</td>\n",
       "      <td>Laser Biostimulation in Periodontal Treatment</td>\n",
       "      <td>https://clinicaltrials.gov/study/NCT04253613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The use of nonsurgical periodontal treatment, ...</td>\n",
       "      <td>NO</td>\n",
       "      <td>Periodontal Inflammation|Periodontal Diseases</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IL-1β level in GCF, IL-1β is a cytokine presen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Observational Model: |Time Perspective: p</td>\n",
       "      <td>OMU KAEK2012/49</td>\n",
       "      <td>1/15/13</td>\n",
       "      <td>6/15/14</td>\n",
       "      <td>6/15/14</td>\n",
       "      <td>02-05-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07-06-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NCT Number                                        Study Title  \\\n",
       "0  NCT03162666        Patient Outcomes Using an Expandable Spacer   \n",
       "1  NCT04312048  the Effect of Isosorbide Mononitrate in Reduci...   \n",
       "2  NCT03144778  Durvalumab With or Without Tremelimumab in Tre...   \n",
       "3  NCT01592721  Radiation and Cetuximab Plus Intratumoral EGFR...   \n",
       "4  NCT04253613      Laser Biostimulation in Periodontal Treatment   \n",
       "\n",
       "                                      Study URL Acronym  \\\n",
       "0  https://clinicaltrials.gov/study/NCT03162666     NaN   \n",
       "1  https://clinicaltrials.gov/study/NCT04312048     NaN   \n",
       "2  https://clinicaltrials.gov/study/NCT03144778     NaN   \n",
       "3  https://clinicaltrials.gov/study/NCT01592721     NaN   \n",
       "4  https://clinicaltrials.gov/study/NCT04253613     NaN   \n",
       "\n",
       "                                       Brief Summary Study Results  \\\n",
       "0  This study is a post-market clinical follow-up...            NO   \n",
       "1  The aim of the study is to evaluate the effica...            NO   \n",
       "2  This phase I trial studies how well durvalumab...            NO   \n",
       "3  The incorporation of novel targeted therapies ...           YES   \n",
       "4  The use of nonsurgical periodontal treatment, ...            NO   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0                          Degenerative Disc Disease   \n",
       "1                                 IUD Insertion Pain   \n",
       "2  Stage II Oropharyngeal Squamous Cell Carcinoma...   \n",
       "3       Squamous Cell Carcinoma|Head and Neck Cancer   \n",
       "4      Periodontal Inflammation|Periodontal Diseases   \n",
       "\n",
       "                                     Interventions  \\\n",
       "0                                   DEVICE: ALTERA   \n",
       "1       DRUG: Isosorbide mononitrate|DRUG: placebo   \n",
       "2  BIOLOGICAL: Durvalumab|BIOLOGICAL: Tremelimumab   \n",
       "3                   BIOLOGICAL: EGFR Antisense DNA   \n",
       "4                                              NaN   \n",
       "\n",
       "                            Primary Outcome Measures  \\\n",
       "0  Change in Radiographic Analysis, Global and Se...   \n",
       "1  pain during IUD insertion, intensity of patien...   \n",
       "2  Change of CD8+ tumor infiltrating lymphocytes,...   \n",
       "3  Toxicity Rate, This is a 2-stage clinical tria...   \n",
       "4  IL-1β level in GCF, IL-1β is a cytokine presen...   \n",
       "\n",
       "                          Secondary Outcome Measures  ...  \\\n",
       "0  Complications, Surgical or device related comp...  ...   \n",
       "1  duration of IUD insertion, duration of IUD ins...  ...   \n",
       "2  Incidence of adverse events and serious advers...  ...   \n",
       "3  Tumor Response, Clinical secondary endpoints i...  ...   \n",
       "4                                                NaN  ...   \n",
       "\n",
       "                                        Study Design  \\\n",
       "0          Observational Model: |Time Perspective: p   \n",
       "1  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "2  Allocation: RANDOMIZED|Intervention Model: PAR...   \n",
       "3  Allocation: NA|Intervention Model: SINGLE_GROU...   \n",
       "4          Observational Model: |Time Perspective: p   \n",
       "\n",
       "                            Other IDs Start Date Primary Completion Date  \\\n",
       "0                           RGC16-001    2/28/17                11-06-20   \n",
       "1                                 249    4/15/20                10/25/20   \n",
       "2  2016-0805|NCI-2018-01199|2016-0805   07-12-17                 3/15/21   \n",
       "3             CTRC 11-47|HSC20120131H    2013-04                02-01-22   \n",
       "4                     OMU KAEK2012/49    1/15/13                 6/15/14   \n",
       "\n",
       "  Completion Date First Posted  Results First Posted Last Update Posted  \\\n",
       "0        11-06-20      5/22/17                   NaN            8/23/21   \n",
       "1        11/30/20      3/18/20                   NaN            1/20/21   \n",
       "2         3/15/21     05-09-17                   NaN            3/18/21   \n",
       "3        02-01-22     05-07-12               9/21/22            9/21/22   \n",
       "4         6/15/14     02-05-20                   NaN           07-06-22   \n",
       "\n",
       "                                           Locations  \\\n",
       "0  Rush University Medical Center, Chicago, Illin...   \n",
       "1                     Ahmed Samy, Giza, 11231, Egypt   \n",
       "2  M D Anderson Cancer Center, Houston, Texas, 77...   \n",
       "3  University of Pittsburgh, Pittsburgh, Pennsylv...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     Study Documents  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  Study Protocol and Statistical Analysis Plan, ...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   2%|▏         | 1024/64395 [07:08<7:22:20,  2.39 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# ✅ Optimized batch processing with map function\u001b[39;00m\n\u001b[0;32m     25\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m---> 26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_classify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mConditions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Convert back to Pandas DataFrame\u001b[39;00m\n\u001b[0;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([data, dataset\u001b[38;5;241m.\u001b[39mto_pandas()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\datasets\\arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\datasets\\arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3454\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3455\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3456\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3458\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3462\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\datasets\\arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3324\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m, in \u001b[0;36mbatch_classify\u001b[1;34m(batch, threshold)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbatch_classify\u001b[39m(batch, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Classifies a batch of conditions into predefined categories.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mConditions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     classified_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m classification \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[1;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\base.py:1283\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1280\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1281\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1282\u001b[0m     )\n\u001b[1;32m-> 1283\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[0;32m    236\u001b[0m }\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1763\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1761\u001b[0m     )\n\u001b[1;32m-> 1763\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1528\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1521\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1522\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1523\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1524\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1525\u001b[0m     )\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1528\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1380\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1368\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1369\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         use_cache,\n\u001b[0;32m   1378\u001b[0m     )\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1380\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1393\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:666\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    664\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    674\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:475\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    472\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# self_attention\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    476\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, bsz)\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ✅ Use GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=device)\n",
    "\n",
    "# ✅ Define categories\n",
    "categories = [\"Oncology\", \"Non-Oncology\"]\n",
    "\n",
    "# ✅ Convert Pandas DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# ✅ Batch processing function (Uses GPU)\n",
    "def batch_classify(batch, threshold=0.7):\n",
    "    \"\"\"Classifies a batch of conditions into predefined categories.\"\"\"\n",
    "    results = classifier(batch['Conditions'], candidate_labels=categories, batch_size=64)\n",
    "\n",
    "    classified_labels = []\n",
    "    for classification in results:\n",
    "        best_label = classification['labels'][0]\n",
    "        best_score = classification['scores'][0]\n",
    "        classified_labels.append(best_label if best_score >= threshold else \"Other Rare or Unclassified\")\n",
    "\n",
    "    return {\"Conditions_Category\": classified_labels}\n",
    "\n",
    "# ✅ Optimized batch processing with map function\n",
    "batch_size = 512\n",
    "dataset = dataset.map(batch_classify, batched=True, batch_size=batch_size, remove_columns=['Conditions'])\n",
    "\n",
    "# Convert back to Pandas DataFrame\n",
    "data = pd.concat([data, dataset.to_pandas()], axis=1)\n",
    "\n",
    "# ✅ Show Completion Status\n",
    "total_rows = len(data)\n",
    "classified_rows = data['Conditions_Category'].notnull().sum()\n",
    "completion_percentage = (classified_rows / total_rows) * 100\n",
    "print(f\"✅ Classification Completed: {completion_percentage:.2f}% of rows processed.\") # Use print instead of tqdm.write for cleaner output\n",
    "\n",
    "# ✅ Display Sample Results\n",
    "print(data[['Conditions', 'Conditions_Category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicate_columns(df):\n",
    "    df_T = df.T.drop_duplicates().T  # Transpose, drop duplicates, and transpose back\n",
    "    return df_T\n",
    "\n",
    "data = drop_duplicate_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all text attributes into a single column 'Unstructured'\n",
    "data[\"Outcomes\"] = data[\n",
    "    [\"Primary Outcome Measures\", \"Secondary Outcome Measures\", \"Other Outcome Measures\"]\n",
    "].astype(str).agg(\" [SEP] \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fine-Tuned Model and Tokenizer\n",
    "\n",
    "model_path = folder_path+ \"ClinicalBioBertFinal/study_title_fine_tuned_clinicalbiobert_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Function to tokenize input texts\n",
    "def encode_texts(texts, tokenizer, max_length=56):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# Function for Model Inference\n",
    "def predict(data, tokenizer, model, device, text_column=\"Study Title\", max_length=56, batch_size=16):\n",
    "    # Ensure column exists\n",
    "    if text_column not in data.columns:\n",
    "        raise KeyError(f\"Column '{text_column}' not found in DataFrame. Available columns: {list(data.columns)}\")\n",
    "\n",
    "    # Convert DataFrame column to list\n",
    "    texts = list(data[text_column].astype(str))\n",
    "    nct_numbers = list(data[\"NCT Number\"].astype(str))  # Ensure ID column is string\n",
    "\n",
    "    # Tokenize texts\n",
    "    encodings = encode_texts(texts, tokenizer, max_length)\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_input_ids, batch_attention_mask = batch\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)  # Get prediction probabilities\n",
    "\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            prediction_probs.extend(batch_probs)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"NCT Number\": nct_numbers,\n",
    "        f\"Predicted Label_{text_column}\": predictions,\n",
    "        f\"Probability Class 0_{text_column}\": [prob[0] for prob in prediction_probs],\n",
    "        f\"Probability Class 1_{text_column}\": [prob[1] for prob in prediction_probs]\n",
    "    })\n",
    "\n",
    "    # Merge with original DataFrame\n",
    "    data = pd.merge(data, pred_df, on=\"NCT Number\", how=\"left\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Run inference on test data\n",
    "data = predict(\n",
    "    data=data,  # Your test dataset\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    text_column=\"Study Title\",\n",
    "    max_length=56,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Display updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = folder_path+ \"ClinicalBioBertFinal/brief_summary_fine_tuned_clinicalbiobert_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Run inference on test data\n",
    "data = predict(\n",
    "    data=data,  # Your test dataset\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    text_column=\"Brief Summary\",\n",
    "    max_length=128,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Display updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = folder_path+ \"ClinicalBioBertFinal/Conditions_fine_tuned_clinicalbiobert_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Run inference on test data\n",
    "data = predict(\n",
    "    data=data,  # Your test dataset\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    text_column=\"Conditions\",\n",
    "    max_length=128,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Display updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = folder_path+ \"ClinicalBioBertFinal/Outcomes_fine_tuned_clinicalbiobert_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Run inference on test data\n",
    "data = predict(\n",
    "    data=data,  # Your test dataset\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    text_column=\"Outcomes\",\n",
    "    max_length=128,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Display updated DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Study URL', 'Acronym', 'Study Results', 'Phases', 'Enrollment', 'Other IDs', 'Primary Completion Date', 'Completion Date', 'First Posted', 'Results First Posted', 'Last Update Posted', 'Study Documents', 'Primary Outcome Measures', 'Secondary Outcome Measures', 'Other Outcome Measures'], errors='ignore')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Start Date is in datetime format\n",
    "data['Start Date'] = pd.to_datetime(data['Start Date'], errors='coerce')\n",
    "\n",
    "# Extract month and quarter, handling missing values\n",
    "data['Start Month'] = data['Start Date'].dt.month.fillna(-1).astype(int).astype('category')\n",
    "data['Start Quarter'] = data['Start Date'].dt.quarter.fillna(-1).astype(int).astype('category')\n",
    "\n",
    "# Verify the changes\n",
    "data[['Start Date', 'Start Month', 'Start Quarter']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of collaborators per trial\n",
    "data['Num_Collaborators'] = data['Collaborators'].apply(lambda x: len(str(x).split('|')) if pd.notna(x) else 0)\n",
    "# Count number of collaborators per trial\n",
    "data['Num_Collaborators'] = data['Collaborators'].apply(lambda x: len(str(x).split('|')) if pd.notna(x) else 0)\n",
    "# Expanded keyword-based classification\n",
    "def refine_collaborator_type(collaborator):\n",
    "    if pd.isna(collaborator) or collaborator.strip() == \"\":\n",
    "        return \"Unknown\"\n",
    "\n",
    "    collaborator = collaborator.lower()\n",
    "\n",
    "    # Pharmaceutical/Biotech Companies\n",
    "    if any(x in collaborator for x in [\"pharma\", \"biotech\", \"therapeutics\", \"biosciences\", \"genomics\", \"biopharma\", \"life sciences\", \"vaccines\", \"therapeutics\"]):\n",
    "        return \"Pharmaceutical/Biotech\"\n",
    "\n",
    "    # Medical Device Companies\n",
    "    elif any(x in collaborator for x in [\"medtech\", \"medical devices\", \"diagnostics\", \"boston scientific\", \"medtronic\", \"abbott\", \"siemens healthineers\", \"imaging\", \"st. jude medical\", \"stryker\"]):\n",
    "        return \"Medical Device Company\"\n",
    "\n",
    "    # Universities & Academic Institutions\n",
    "    elif any(x in collaborator for x in [\"university\", \"college\", \"academy\", \"school of medicine\", \"medical school\"]):\n",
    "        return \"University\"\n",
    "\n",
    "    # Hospitals & Medical Centers\n",
    "    elif any(x in collaborator for x in [\"hospital\", \"medical center\", \"health system\", \"clinic\", \"health network\", \"cancer center\", \"teaching hospital\"]):\n",
    "        return \"Hospital/Medical Center\"\n",
    "\n",
    "    # Academic Research Centers\n",
    "    elif any(x in collaborator for x in [\"institute of technology\", \"research institute\", \"academic research\", \"broad institute\", \"howard hughes\", \"karolinska institute\"]):\n",
    "        return \"Academic Research Institute\"\n",
    "\n",
    "    # Non-Academic Research Centers\n",
    "    elif any(x in collaborator for x in [\"national laboratory\", \"max planck\", \"clinical research\", \"research foundation\"]):\n",
    "        return \"Non-Academic Research Institute\"\n",
    "\n",
    "    # Government & Public Health Agencies\n",
    "    elif any(x in collaborator for x in [\"national\", \"federal\", \"ministry of health\", \"nih\", \"fda\", \"cdc\", \"who\", \"government\", \"department of health\", \"european medicines agency\", \"health canada\"]):\n",
    "        return \"Government\"\n",
    "\n",
    "    # Military & Defense Medical Research\n",
    "    elif any(x in collaborator for x in [\"military\", \"army\", \"navy\", \"air force\", \"veterans affairs\", \"dod\", \"defense\", \"va medical\", \"darpa\", \"us army medical research\"]):\n",
    "        return \"Military/Defense Medical Research\"\n",
    "\n",
    "    # Non-Profit Organizations & NGOs\n",
    "    elif any(x in collaborator for x in [\"non-profit\", \"ngo\", \"humanitarian\", \"relief fund\", \"patient advocacy\", \"community group\", \"alzheimer's association\", \"parkinson's foundation\"]):\n",
    "        return \"Non-Profit / Patient Advocacy\"\n",
    "\n",
    "    # Foundations (Funding Agencies)\n",
    "    elif any(x in collaborator for x in [\"foundation\", \"charitable trust\", \"funding agency\", \"gates foundation\", \"michael j. fox foundation\"]):\n",
    "        return \"Foundation (Funding Organization)\"\n",
    "\n",
    "    # Contract Research Organizations (CROs)\n",
    "    elif any(x in collaborator for x in [\"quintiles\", \"iqvia\", \"parexel\", \"covance\", \"medpace\", \"ppd\", \"icon\", \"charles river\", \"syneos\", \"contract research\"]):\n",
    "        return \"Contract Research Organization (CRO)\"\n",
    "\n",
    "    # Healthcare Consortia & Alliances\n",
    "    elif any(x in collaborator for x in [\"consortium\", \"partnership\", \"collaborative\", \"alliance\", \"global initiative\"]):\n",
    "        return \"Healthcare Consortium/Alliance\"\n",
    "\n",
    "    # If no match, return Other\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Apply classification\n",
    "data['Collaborator_Type'] = data['Collaborators'].apply(refine_collaborator_type)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sex'] = data['Sex'].fillna('ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Collaborators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Sponsor','Age','Funder Type','Study Type','Study Design'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of columns\n",
    "columns_mode = ['Age', 'Sex']\n",
    "columns_unknown = ['Funder Type', 'Study Type', 'Study Design', 'Outcomes']\n",
    "\n",
    "# Fill numerical columns with median\n",
    "for col in columns_mode:\n",
    "    mode_value = data[col].mode()\n",
    "    data[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "# Fill categorical columns with 'unknown'\n",
    "for col in columns_unknown:\n",
    "    data[col].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to classify conditions with detailed categories\n",
    "def classify_condition(condition):\n",
    "    if pd.isnull(condition):\n",
    "        return 'Unknown'\n",
    "\n",
    "    condition_lower = condition.lower()\n",
    "\n",
    "    # Cancer-related conditions\n",
    "    if any(keyword in condition_lower for keyword in ['cancer', 'tumor', 'carcinoma', 'lymphoma', 'leukemia', 'melanoma']):\n",
    "        return 'Cancer'\n",
    "\n",
    "    # Cardiovascular diseases\n",
    "    if any(keyword in condition_lower for keyword in ['heart', 'cardio', 'vascular', 'stroke', 'artery', 'hypertension', 'atrial']):\n",
    "        return 'Cardiovascular'\n",
    "\n",
    "    # Neurological disorders\n",
    "    if any(keyword in condition_lower for keyword in ['brain', 'neuro', 'alzheimer', 'parkinson', 'epilepsy', 'multiple sclerosis']):\n",
    "        return 'Neurological'\n",
    "\n",
    "    # Infectious diseases\n",
    "    if any(keyword in condition_lower for keyword in ['infection', 'virus', 'bacterial', 'hiv', 'malaria', 'covid', 'influenza']):\n",
    "        return 'Infectious Diseases'\n",
    "\n",
    "    # Endocrine and metabolic disorders\n",
    "    if any(keyword in condition_lower for keyword in ['diabetes', 'thyroid', 'obesity', 'metabolic', 'insulin']):\n",
    "        return 'Endocrine/Metabolic'\n",
    "\n",
    "    # Respiratory conditions\n",
    "    if any(keyword in condition_lower for keyword in ['asthma', 'lung', 'copd', 'pulmonary']):\n",
    "        return 'Respiratory'\n",
    "\n",
    "    # Gastrointestinal & Hepatic diseases\n",
    "    if any(keyword in condition_lower for keyword in ['liver', 'hepatic', 'pancreatic', 'colorectal', 'gastro']):\n",
    "        return 'Gastrointestinal & Hepatic'\n",
    "\n",
    "    # Autoimmune and inflammatory conditions\n",
    "    if any(keyword in condition_lower for keyword in ['arthritis', 'rheumatoid', 'psoriasis', 'inflammatory', 'lupus', 'crohn']):\n",
    "        return 'Autoimmune & Inflammatory'\n",
    "\n",
    "    # Musculoskeletal disorders\n",
    "    if any(keyword in condition_lower for keyword in ['osteoporosis', 'osteoarthritis', 'fracture', 'spine', 'back pain', 'knee']):\n",
    "        return 'Musculoskeletal'\n",
    "\n",
    "    # Psychiatric & behavioral disorders\n",
    "    if any(keyword in condition_lower for keyword in ['schizophrenia', 'bipolar', 'depression', 'psychiatric', 'mental health']):\n",
    "        return 'Psychiatric & Behavioral'\n",
    "\n",
    "    # Reproductive & urological disorders\n",
    "    if any(keyword in condition_lower for keyword in ['infertility', 'prostate', 'kidney', 'renal', 'bladder', 'reproductive']):\n",
    "        return 'Reproductive & Urological'\n",
    "\n",
    "    # General health & other\n",
    "    if any(keyword in condition_lower for keyword in ['healthy', 'general health', 'volunteers']):\n",
    "        return 'General Health & Others'\n",
    "\n",
    "    # Default to \"Other\" for unspecified conditions\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the improved classification function\n",
    "data['Condition_Category_old'] = data['Conditions'].apply(classify_condition)\n",
    "\n",
    "# Verify results\n",
    "data[['Conditions', 'Condition_Category_old']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Conditions'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify sponsors\n",
    "def classify_sponsor(sponsor):\n",
    "    if pd.isnull(sponsor):\n",
    "        return 'Unknown'\n",
    "    sponsor_lower = sponsor.lower()\n",
    "\n",
    "    # University\n",
    "    if 'university' in sponsor_lower or 'school of medicine' in sponsor_lower:\n",
    "        return 'University'\n",
    "\n",
    "    # Hospital or Clinic\n",
    "    if 'hospital' in sponsor_lower or 'clinic' in sponsor_lower or 'center' in sponsor_lower:\n",
    "        return 'Hospital/Clinic'\n",
    "\n",
    "    # Company\n",
    "    company_keywords = ['pharmaceuticals', 'biopharma', 'biosciences', 'llc', 'inc',\n",
    "                         'company', 'a/s', 'ltd', 'plc', 'corporation']\n",
    "    if any(keyword in sponsor_lower for keyword in company_keywords):\n",
    "        return 'Company'\n",
    "\n",
    "    # Government Institute\n",
    "    gov_keywords = ['national', 'institute', 'office of research', 'va', 'nci', 'niaid']\n",
    "    if any(keyword in sponsor_lower for keyword in gov_keywords):\n",
    "        return 'Government Institute'\n",
    "\n",
    "    # Other\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the classification function to the Sponsor column\n",
    "data['Sponsor Type'] = data['Sponsor'].apply(classify_sponsor)\n",
    "data.drop(columns=['Sponsor'], inplace=True)\n",
    "# Verify the distribution of Sponsor Type\n",
    "print(data['Sponsor Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique age categories\n",
    "age_categories = ['CHILD', 'ADULT', 'OLDER_ADULT']\n",
    "\n",
    "# Create binary columns for each age group\n",
    "for category in age_categories:\n",
    "    data[category] = data['Age'].apply(lambda x: 1 if isinstance(x, str) and category in x else 0)\n",
    "\n",
    "# Drop the original 'Age' column\n",
    "data.drop(columns=['Age'], inplace=True)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attribute values\n",
    "def extract_attribute(study_design, attribute):\n",
    "    if pd.isnull(study_design):\n",
    "        return None\n",
    "    for part in study_design.split('|'):\n",
    "        if attribute in part:\n",
    "            return part.split(':')[1].strip() if ':' in part else None\n",
    "    return None\n",
    "\n",
    "# Extract key features\n",
    "data['Allocation'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Allocation'))\n",
    "data['Intervention Model'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Intervention Model'))\n",
    "data['Masking'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Masking'))\n",
    "data['Primary Purpose'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Primary Purpose'))\n",
    "data['Observational Model'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Observational Model'))\n",
    "data['Time Perspective'] = data['Study Design'].apply(lambda x: extract_attribute(x, 'Time Perspective'))\n",
    "\n",
    "# Extract Masking Level\n",
    "def extract_masking_level(masking):\n",
    "    if pd.isnull(masking):\n",
    "        return 'NONE'\n",
    "    if 'QUADRUPLE' in masking:\n",
    "        return 'QUADRUPLE'\n",
    "    if 'TRIPLE' in masking:\n",
    "        return 'TRIPLE'\n",
    "    if 'DOUBLE' in masking:\n",
    "        return 'DOUBLE'\n",
    "    if 'SINGLE' in masking:\n",
    "        return 'SINGLE'\n",
    "    return 'NONE'\n",
    "\n",
    "data['Masking Level'] = data['Masking'].apply(extract_masking_level)\n",
    "\n",
    "# Extract Masking Details\n",
    "def extract_masking_details(masking):\n",
    "    if pd.isnull(masking):\n",
    "        return 'NONE'\n",
    "    if '(' in masking:\n",
    "        return masking.split('(')[1].strip(')')\n",
    "    return 'NONE'\n",
    "\n",
    "data['Masking Details'] = data['Masking'].apply(extract_masking_details)\n",
    "\n",
    "# Handle missing values by filling with 'Unknown'\n",
    "data['Allocation'].fillna('Unknown', inplace=True)\n",
    "data['Intervention Model'].fillna('Unknown', inplace=True)\n",
    "data['Primary Purpose'].fillna('Unknown', inplace=True)\n",
    "data['Observational Model'].fillna('Unknown', inplace=True)\n",
    "data['Time Perspective'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "data = pd.get_dummies(\n",
    "    data,\n",
    "    columns=['Allocation', 'Intervention Model', 'Masking Level', 'Primary Purpose']\n",
    ")\n",
    "\n",
    "# Drop original 'Study Design' column\n",
    "# data.drop(columns=['Study Design'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with empty strings before concatenation\n",
    "data['Combined_Text'] = (\n",
    "    data['Study Title'].fillna('') + ' ' +\n",
    "    data['Brief Summary'].fillna('') + ' ' +\n",
    "    data['Outcomes'].fillna('')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wordnet and stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Required for wordnet lemmatizer\n",
    "nltk.download('stopwords')  # Ensure stopwords are available\n",
    "\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagging\n",
    "nltk.download('wordnet')  # Lemmatization\n",
    "nltk.download('stopwords')  # Stopwords\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 195 countries from the dataset\n",
    "country_list = [\n",
    "    \"India\", \"China\", \"United States\", \"Indonesia\", \"Pakistan\", \"Nigeria\", \"Brazil\", \"Bangladesh\", \"Russia\",\n",
    "    \"Ethiopia\", \"Mexico\", \"Japan\", \"Egypt\", \"Philippines\", \"DR Congo\", \"Vietnam\", \"Iran\", \"Turkey\", \"Germany\",\n",
    "    \"Thailand\", \"United Kingdom\", \"Tanzania\", \"France\", \"South Africa\", \"Italy\", \"Kenya\", \"Myanmar\", \"Colombia\",\n",
    "    \"Korea\", \"Sudan\", \"Uganda\", \"Spain\", \"Algeria\", \"Iraq\", \"Argentina\", \"Afghanistan\", \"Yemen\", \"Canada\",\n",
    "    \"Poland\", \"Morocco\", \"Angola\", \"Ukraine\", \"Uzbekistan\", \"Malaysia\", \"Mozambique\", \"Ghana\", \"Peru\", \"Saudi Arabia\",\n",
    "    \"Madagascar\", \"Côte D'Ivoire\", \"Nepal\", \"Cameroon\", \"Venezuela\", \"Niger\", \"Australia\", \"North Korea\", \"Syria\",\n",
    "    \"Mali\", \"Burkina Faso\", \"Sri Lanka\", \"Malawi\", \"Zambia\", \"Kazakhstan\", \"Chad\", \"Chile\", \"Romania\", \"Somalia\",\n",
    "    \"Senegal\", \"Guatemala\", \"Netherlands\", \"Ecuador\", \"Cambodia\", \"Zimbabwe\", \"Guinea\", \"Benin\", \"Rwanda\", \"Burundi\",\n",
    "    \"Bolivia\", \"Tunisia\", \"South Sudan\", \"Haiti\", \"Belgium\", \"Jordan\", \"Dominican Republic\", \"United Arab Emirates\",\n",
    "    \"Cuba\", \"Honduras\", \"Czech Republic\", \"Sweden\", \"Tajikistan\", \"Papua New Guinea\", \"Portugal\", \"Azerbaijan\",\n",
    "    \"Greece\", \"Hungary\", \"Togo\", \"Israel\", \"Austria\", \"Belarus\", \"Switzerland\", \"Sierra Leone\", \"Laos\", \"Turkmenistan\",\n",
    "    \"Libya\", \"Kyrgyzstan\", \"Paraguay\", \"Nicaragua\", \"Bulgaria\", \"Serbia\", \"El Salvador\", \"Congo\", \"Denmark\",\n",
    "    \"Singapore\", \"Lebanon\", \"Finland\", \"Liberia\", \"Norway\", \"Slovakia\", \"State of Palestine\",\n",
    "    \"Central African Republic\", \"Oman\", \"Ireland\", \"New Zealand\", \"Mauritania\", \"Costa Rica\", \"Kuwait\", \"Panama\",\n",
    "    \"Croatia\", \"Georgia\", \"Eritrea\", \"Mongolia\", \"Uruguay\", \"Bosnia and Herzegovina\", \"Qatar\", \"Moldova\", \"Namibia\",\n",
    "    \"Armenia\", \"Lithuania\", \"Jamaica\", \"Albania\", \"Gambia\", \"Gabon\", \"Botswana\", \"Lesotho\", \"Guinea-Bissau\",\n",
    "    \"Slovenia\", \"Equatorial Guinea\", \"Latvia\", \"North Macedonia\", \"Bahrain\", \"Trinidad and Tobago\", \"Timor-Leste\",\n",
    "    \"Estonia\", \"Cyprus\", \"Mauritius\", \"Eswatini\", \"Djibouti\", \"Fiji\", \"Comoros\", \"Guyana\", \"Solomon Islands\",\n",
    "    \"Bhutan\", \"Luxembourg\", \"Montenegro\", \"Suriname\", \"Malta\", \"Maldives\", \"Micronesia\", \"Cabo Verde\", \"Brunei\",\n",
    "    \"Belize\", \"Bahamas\", \"Iceland\", \"Vanuatu\", \"Barbados\", \"Sao Tome & Principe\", \"Samoa\", \"Saint Lucia\", \"Kiribati\",\n",
    "    \"Seychelles\", \"Grenada\", \"Tonga\", \"St. Vincent & Grenadines\", \"Antigua and Barbuda\", \"Andorra\", \"Dominica\",\n",
    "    \"Saint Kitts & Nevis\", \"Liechtenstein\", \"Monaco\", \"Marshall Islands\", \"San Marino\", \"Palau\", \"Nauru\", \"Tuvalu\",\n",
    "    \"Holy See\",\"Taiwan\",\"Hong Kong\",\"Czechia\",\"Puerto Rico\",\"Palestinian Territory\",\"Swaziland\",\"Faroe Islands\",\"Réunion\",\"Yugoslavia\",\"French Guiana\",\"Lao People's Democratic Republic\",\"The Former Yugoslav Republic of\",\"GSK Investigational Site\",\"Guadeloupe\",\"French Polynesia\",\"Kosovo\",\"Macau\",\"Guam\",\"Greenland\",\"Saint Kitts and Nevis\", \"New Caledonia\", \"Northern Mariana Islands\", \"New Caledonia\", \"Novartis Investigative Site\", \"Palestinian Territories\", \"Cayman Islands\"\n",
    "]\n",
    "\n",
    "\n",
    "# Function to extract state and country from location text\n",
    "def extract_state_country(location):\n",
    "    location = str(location)  # Ensure it's a string\n",
    "    country = None\n",
    "    state = None\n",
    "\n",
    "    # Extract country from location text\n",
    "    for c in country_list:\n",
    "        if c in location:\n",
    "            country = c\n",
    "            break\n",
    "\n",
    "    # Extract state (for US locations)\n",
    "    location_parts = re.split(r'[|,]', location)  # Split by '|' or ','\n",
    "    for part in location_parts:\n",
    "        part = part.strip()\n",
    "    return country\n",
    "\n",
    "# Apply function to extract state and country\n",
    "data[['Country']] = data['Locations'].apply(lambda x: pd.Series(extract_state_country(x)))\n",
    "\n",
    "# Display results\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Country.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "developed = [\n",
    "    \"United States\", \"Canada\", \"Germany\", \"Japan\", \"United Kingdom\", \"France\", \"Switzerland\",\n",
    "    \"Netherlands\", \"Sweden\", \"Denmark\", \"Norway\", \"Finland\", \"Australia\", \"New Zealand\", \"Belgium\",\n",
    "    \"Austria\", \"Ireland\", \"Singapore\", \"Hong Kong\", \"Taiwan\", \"Israel\", \"Luxembourg\",\n",
    "    \"Iceland\", \"Czech Republic\", \"Estonia\", \"Slovenia\", \"Portugal\",\"Spain\", \"Italy\", \"Czechia\", \"Korea\"\n",
    "]\n",
    "\n",
    "developing = [\n",
    "    \"India\", \"China\", \"Brazil\", \"Russia\", \"Mexico\", \"South Africa\", \"Indonesia\", \"Malaysia\", \"Turkey\",\n",
    "    \"Argentina\", \"Thailand\", \"Vietnam\", \"Chile\", \"Colombia\", \"Poland\", \"Hungary\", \"Slovakia\", \"Lithuania\",\n",
    "    \"Latvia\", \"Kazakhstan\", \"Bulgaria\", \"Romania\", \"Peru\", \"Uruguay\", \"Panama\", \"United Arab Emirates\",\n",
    "    \"Costa Rica\", \"Qatar\", \"Oman\", \"Saudi Arabia\", \"Malta\", \"Bahrain\", \"Kuwait\",\"Egypt\", \"Pakistan\", \"Ethiopia\", \"Iraq\", \"Kenya\", \"Jordan\", \"Iran\",\n",
    "    \"Cameroon\", \"Greece\", \"Philippines\", \"Tanzania\", \"Kyrgyzstan\", \"Senegal\", \"Tunisia\",\n",
    "    \"Belarus\", \"Croatia\", \"Cyprus\", \"Lebanon\", \"Ghana\", \"Mauritius\", \"Jamaica\", \"Uganda\",\n",
    "    \"Serbia\", \"Armenia\", \"Guatemala\", \"Ukraine\", \"Nigeria\", \"Algeria\", \"Cuba\", \"Bolivia\",\n",
    "    \"El Salvador\", \"Bosnia and Herzegovina\", \"Venezuela\", \"Moldova\", \"Georgia\", \"Paraguay\",\n",
    "    \"Botswana\", \"Morocco\", \"Sri Lanka\", \"Trinidad and Tobago\", \"Brunei\", \"Uzbekistan\",\n",
    "    \"Honduras\", \"Ecuador\", \"Bhutan\", \"Montenegro\", \"Albania\", \"Fiji\", \"Bahamas\", \"Suriname\",\n",
    "    \"North Macedonia\", \"Nicaragua\", \"Mongolia\", \"Azerbaijan\"\n",
    "]\n",
    "\n",
    "underdeveloped = [\n",
    "    \"Afghanistan\", \"Yemen\", \"Sudan\", \"Chad\", \"Somalia\", \"South Sudan\", \"Democratic Republic of Congo\",\n",
    "    \"Haiti\", \"Burundi\", \"Liberia\", \"Sierra Leone\", \"Mozambique\", \"Madagascar\", \"Central African Republic\",\n",
    "    \"Eritrea\", \"Niger\", \"Mali\", \"Burkina Faso\", \"Guinea\", \"Guinea-Bissau\", \"Gambia\", \"Comoros\", \"Zambia\",\n",
    "    \"Zimbabwe\", \"Nepal\", \"Bangladesh\", \"Myanmar\", \"Papua New Guinea\", \"Rwanda\", \"Congo\", \"Syria\", \"Malawi\", \"Benin\", \"Gabon\", \"Cambodia\", \"Lesotho\", \"Dominican Republic\",\n",
    "    \"Lao People's Democratic Republic\", \"Namibia\", \"Guyana\", \"Libya\"\n",
    "]\n",
    "#Other Institutions (Territories, Former Countries, and Research Sites)\n",
    "other_institutions = [\n",
    "    \"Puerto Rico\", \"Palestinian Territory\", \"Réunion\", \"French Guiana\", \"Macau\", \"Guadeloupe\",\n",
    "    \"Kosovo\", \"Guam\", \"Greenland\", \"French Polynesia\", \"Faroe Islands\", \"Saint Kitts and Nevis\",\n",
    "    \"Andorra\", \"San Marino\", \"Micronesia\", \"Solomon Islands\", \"Vanuatu\", \"Samoa\", \"Saint Lucia\",\n",
    "    \"Grenada\", \"Barbados\", \"Antigua and Barbuda\", \"Liechtenstein\", \"GSK Investigational Site\",\n",
    "    \"Yugoslavia\", \"The Former Yugoslav Republic of\", \"New Caledonia\", \"Northern Mariana Islands\", \"New Caledonia\", \"Novartis Investigative Site\", \"Palestinian Territories\", \"Cayman Islands\",\"Monaco\", \"Côte D'Ivoire\", \"Swaziland\"\n",
    "]\n",
    "\n",
    "\n",
    "# Define category mappings\n",
    "def categorize_country(country):\n",
    "    if country in developed:\n",
    "        return \"Developed\"\n",
    "    elif country in developing:\n",
    "        return \"Developing\"\n",
    "    elif country in underdeveloped:\n",
    "        return \"Underdeveloped\"\n",
    "    elif country in other_institutions:\n",
    "        return \"Other Institutions\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Apply country categorization\n",
    "data['Development Category'] = data['Country'].apply(categorize_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Development Category'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Development Category'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute linguistic features\n",
    "def extract_text_features(text):\n",
    "    if pd.isnull(text) or text.strip() == '':\n",
    "        return pd.Series([0, 0, 0, 0, 0])  # Handle missing values\n",
    "\n",
    "    # Word Count\n",
    "    word_count = len(text.split())\n",
    "\n",
    "    # Character Count (excluding spaces)\n",
    "    char_count = len(text.replace(\" \", \"\"))\n",
    "\n",
    "    # Sentence Count (Using TextBlob instead of nltk)\n",
    "    sentence_count = len(TextBlob(text).sentences)\n",
    "\n",
    "    # Average Word Length\n",
    "    avg_word_length = char_count / word_count if word_count > 0 else 0\n",
    "\n",
    "    # Sentiment Score\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "\n",
    "    return pd.Series([word_count, char_count, sentence_count, avg_word_length, sentiment])\n",
    "\n",
    "# Apply feature extraction to each column\n",
    "for col in ['Study Title', 'Brief Summary', 'Outcomes']:\n",
    "    feature_names = [f\"{col}_Word_Count\", f\"{col}_Char_Count\", f\"{col}_Sentence_Count\",\n",
    "                     f\"{col}_Avg_Word_Length\", f\"{col}_Sentiment_Score\"]\n",
    "\n",
    "    data[feature_names] = data[col].apply(extract_text_features)\n",
    "\n",
    "# Verify the extracted features\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Combined_Text', 'Outcomes', 'Study Title', 'Brief Summary', 'Study Design'], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Locations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns (object or category type)\n",
    "categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Get numerical columns (int, float type)\n",
    "numerical_columns = data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Remove 'NCT Number' and 'Study Status' from the lists\n",
    "categorical_columns = [col for col in categorical_columns if col not in ['NCT Number', 'Study Status', 'data_split', 'data_type']]\n",
    "numerical_columns = [col for col in numerical_columns if col not in ['NCT Number', 'Study Status', 'data_split', 'data_type', 'Start Month', 'Start Quarter']]\n",
    "\n",
    "# Explicitly add 'Start Month' and 'Start Quarter' to the categorical columns\n",
    "categorical_columns.extend(['Start Month', 'Start Quarter'])\n",
    "\n",
    "# Optionally, print the lists of categorical and numerical columns\n",
    "print(\"Categorical Columns:\", categorical_columns)\n",
    "print(\"Numerical Columns:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top 5 countries\n",
    "top_5_countries = ['United States', 'Unknown', 'France', 'India', 'China']\n",
    "\n",
    "# Create separate columns for each of the top 5 countries\n",
    "for country in top_5_countries:\n",
    "    data[country] = data['Country'].apply(lambda x: 1 if x == country else 0)\n",
    "\n",
    "data = data.drop(columns=['Country'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in categorical_columns if col not in ['NCT Number', 'Study Status', 'data_split', 'data_type', 'Country', 'Study Type', 'categorical_columns']]\n",
    "numerical_columns.extend(['United States', 'Unknown', 'France', 'India', 'China'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Conditions_Category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved OneHotEncoder\n",
    "encoder = joblib.load(folder_path + 'one_hot_encoder.pkl')\n",
    "\n",
    "# Load the original feature names from training\n",
    "with open(folder_path + 'one_hot_encoded_feature_names.json', 'r') as f:\n",
    "    original_feature_names = json.load(f)\n",
    "\n",
    "# Identify categorical columns used during training\n",
    "categorical_columns = list(encoder.feature_names_in_)\n",
    "\n",
    "# Ensure categorical columns exist in new data\n",
    "missing_features = [col for col in categorical_columns if col not in data.columns]\n",
    "if missing_features:\n",
    "    raise ValueError(f\"Missing categorical columns in new data: {missing_features}\")\n",
    "\n",
    "# Transform categorical columns\n",
    "encoded_columns = encoder.transform(data[categorical_columns])\n",
    "\n",
    "# Create a DataFrame for the encoded categorical features\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Drop original categorical columns and retain numerical ones\n",
    "numerical_columns = [col for col in data.columns if col not in categorical_columns]  # Keep numerical features\n",
    "new_data_encoded = data[numerical_columns].reset_index(drop=True)\n",
    "new_data_encoded = pd.concat([new_data_encoded, encoded_df], axis=1)\n",
    "\n",
    "# Ensure all expected features are present (fill missing ones with 0)\n",
    "for feature in original_feature_names:\n",
    "    if feature not in new_data_encoded.columns:\n",
    "        new_data_encoded[feature] = 0  # Add missing columns with default value\n",
    "\n",
    "# Ensure correct column order before passing to model\n",
    "new_data_encoded = new_data_encoded[original_feature_names]\n",
    "\n",
    "# **Add 'Study Type' back to the final dataset**\n",
    "if 'Study Type' in data.columns:\n",
    "    new_data_encoded['Study Type'] = data['Study Type'].values\n",
    "else:\n",
    "    print(\"Warning: 'Study Type' column not found in original data.\")\n",
    "\n",
    "# **Add 'Study Type' back to the final dataset**\n",
    "if 'Conditions_Category' in data.columns:\n",
    "    new_data_encoded['Conditions_Category'] = data['Conditions_Category'].values\n",
    "else:\n",
    "    print(\"Warning: 'Conditions_Category' column not found in original data.\")\n",
    "\n",
    "# Print final feature verification\n",
    "print(f\"Final Transformed Data Columns: {list(new_data_encoded.columns)}\")\n",
    "new_data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_encoded['Study Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_encoded['Conditions_Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import joblib\n",
    "\n",
    "# # Load the saved OneHotEncoder\n",
    "# encoder = joblib.load(folder_path + 'one_hot_encoder.pkl')\n",
    "\n",
    "# # Ensure categorical columns exist in new_data\n",
    "# categorical_columns = encoder.feature_names_in_  # Fetch the column names used during training\n",
    "\n",
    "# # Transform the categorical columns in new_data\n",
    "# encoded_columns = encoder.transform(data[categorical_columns])\n",
    "\n",
    "# # Create a DataFrame with the one-hot encoded values\n",
    "# encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# # Drop the original categorical columns and append the encoded columns\n",
    "# new_data_encoded = data.drop(columns=categorical_columns, errors='ignore')\n",
    "# new_data_encoded = pd.concat([new_data_encoded, encoded_df], axis=1)\n",
    "\n",
    "# # Display the first few rows of the transformed data\n",
    "# print(new_data_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_data_encoded.columns:\n",
    "    print(f\"{col}: {new_data_encoded[col].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subsets based on Study Type and Conditions_Category\n",
    "model_interventional_oncology = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'INTERVENTIONAL') & (new_data_encoded['Conditions_Category'] == 'Oncology')\n",
    "]\n",
    "\n",
    "model_interventional_non_oncology = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'INTERVENTIONAL') & (new_data_encoded['Conditions_Category'] == 'Non-Oncology')\n",
    "]\n",
    "\n",
    "model_interventional_other = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'INTERVENTIONAL') & (new_data_encoded['Conditions_Category'] == 'Other Rare or Unclassified')\n",
    "]\n",
    "\n",
    "model_observational_oncology = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'OBSERVATIONAL') & (new_data_encoded['Conditions_Category'] == 'Oncology')\n",
    "]\n",
    "\n",
    "model_observational_non_oncology = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'OBSERVATIONAL') & (new_data_encoded['Conditions_Category'] == 'Non-Oncology')\n",
    "]\n",
    "\n",
    "model_observational_other = new_data_encoded[\n",
    "    (new_data_encoded['Study Type'] == 'OBSERVATIONAL') & (new_data_encoded['Conditions_Category'] == 'Other Rare or Unclassified')\n",
    "]\n",
    "\n",
    "# Print counts for each dataset\n",
    "print(f\"Model 1 - Interventional Oncology: {len(model_interventional_oncology)}\")\n",
    "print(f\"Model 2 - Interventional Non-Oncology: {len(model_interventional_non_oncology)}\")\n",
    "print(f\"Model 3 - Interventional Other: {len(model_interventional_other)}\")\n",
    "print(f\"Model 4 - Observational Oncology: {len(model_observational_oncology)}\")\n",
    "print(f\"Model 5 - Observational Non-Oncology: {len(model_observational_non_oncology)}\")\n",
    "print(f\"Model 6 - Observational Other: {len(model_observational_other)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'data_type' for train datasets\n",
    "model_interventional_non_oncology['data_type'] = 'model_interventional_non_oncology'\n",
    "model_interventional_oncology['data_type'] = 'model_interventional_oncology'\n",
    "model_interventional_other['data_type'] = 'model_interventional_other'\n",
    "model_observational_non_oncology['data_type'] = 'model_observational_non_oncology'\n",
    "model_observational_oncology['data_type'] = 'model_observational_oncology'\n",
    "model_observational_other['data_type'] = 'model_observational_other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the DataFrames into one single dataset\n",
    "combined_data = pd.concat([\n",
    "    model_interventional_non_oncology, model_interventional_oncology, model_interventional_other,\n",
    "    model_observational_non_oncology, model_observational_oncology, model_observational_other,\n",
    "], ignore_index=True)\n",
    "\n",
    "# Drop columns that start with 'Unnamed'\n",
    "combined_data = combined_data.loc[:, ~combined_data.columns.str.startswith('Unnamed')]\n",
    "\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Define directory containing saved models and features\n",
    "model_dir = folder_path+ '/models'\n",
    "\n",
    "# List of data types (same as used during training)\n",
    "data_types = [\n",
    "    'model_interventional_non_oncology', 'model_interventional_oncology', 'model_interventional_other',\n",
    "    'model_observational_non_oncology', 'model_observational_oncology', 'model_observational_other'\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize list to store results\n",
    "inference_results = []\n",
    "\n",
    "# Loop over each model corresponding to a data type\n",
    "for data_type in data_types:\n",
    "    # Load the trained model\n",
    "    model_filename = os.path.join(model_dir, f\"{data_type}_xgboost_model.json\")\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(model_filename)\n",
    "\n",
    "    # Load feature names used during training\n",
    "    feature_filename = os.path.join(model_dir, f\"{data_type}_features.json\")\n",
    "    with open(feature_filename, 'r') as f:\n",
    "        feature_names = json.load(f)\n",
    "\n",
    "    # Filter the new test data to match the data type\n",
    "    test_subset = combined_data[combined_data['data_type'] == data_type].copy()\n",
    "    \n",
    "    # Ensure test data contains the necessary features\n",
    "    missing_features = [col for col in feature_names if col not in test_subset.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing expected features for {data_type}: {missing_features}\")\n",
    "\n",
    "    # Select only the trained model features\n",
    "    X_test = test_subset[feature_names]\n",
    "\n",
    "    # Convert to XGBoost DMatrix\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(dtest)  # Probabilities\n",
    "\n",
    "    # Ensure binary classification probability format\n",
    "    if y_pred_proba.ndim == 1:  # Single-class probabilities\n",
    "        y_pred_proba = np.vstack([1 - y_pred_proba, y_pred_proba]).T  # Convert to two-column format\n",
    "\n",
    "    # Convert probabilities to class predictions\n",
    "    y_pred_class = np.round(y_pred_proba[:, 1])  # Class 1 prediction threshold at 0.5\n",
    "\n",
    "    # Create DataFrame for predictions\n",
    "    predictions_df = pd.DataFrame({\n",
    "        \"NCT Number\": test_subset[\"NCT Number\"],\n",
    "        \"Predicted Class\": y_pred_class,\n",
    "        \"Probability Class 0\": y_pred_proba[:, 0],\n",
    "        \"Probability Class 1\": y_pred_proba[:, 1],\n",
    "        \"data_type\": data_type\n",
    "    })\n",
    "\n",
    "    # Append to results\n",
    "    inference_results.append(predictions_df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_predictions = pd.concat(inference_results, ignore_index=True)\n",
    "\n",
    "final_predictions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.filter(regex=r'^(Predicted Label|NCT Number)', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data= pd.merge(filtered_data, final_predictions, on='NCT Number', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'NCT Number',\n",
    "    'Predicted Label_Study Title',\n",
    "    'Predicted Label_Brief Summary',\n",
    "    'Predicted Label_Conditions',\n",
    "    'Predicted Label_Outcomes',\n",
    "    'Predicted Class'\n",
    "]\n",
    "\n",
    "# Select only the desired columns\n",
    "final_data = merged_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store final predictions and probabilities\n",
    "final_predictions = []\n",
    "final_probabilities = []  # List to store probabilities for AUC-ROC\n",
    "\n",
    "# Iterate through each row in the test data\n",
    "for index, row in final_data.iterrows():\n",
    "    # Get the predicted labels for all the features\n",
    "    pred_conditions = row['Predicted Label_Conditions']\n",
    "    pred_study_title = row['Predicted Label_Study Title']\n",
    "    pred_outcomes = row['Predicted Label_Outcomes']\n",
    "    pred_brief_summary = row['Predicted Label_Brief Summary']\n",
    "    pred_others = row['Predicted Class']\n",
    "\n",
    "    # Apply the given logic:\n",
    "    if pred_conditions == 0:\n",
    "        final_predictions.append(0)\n",
    "    else:\n",
    "        # Check if 4/5 of the remaining predictions are 0\n",
    "        remaining_predictions = [pred_study_title, pred_outcomes, pred_brief_summary, pred_others]\n",
    "        zero_count = remaining_predictions.count(0)\n",
    "\n",
    "        if zero_count >= 4:  # 4 or more predictions are 0\n",
    "            final_predictions.append(0)\n",
    "        else:\n",
    "            final_predictions.append(1)\n",
    "\n",
    "# Append the final predictions as a new column to \"final_data\"\n",
    "final_data[\"Final Prediction_Max_Voting\"] = final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the final majority voting predictions\n",
    "final_predictions = []\n",
    "\n",
    "# Iterate through each row in the test data\n",
    "for index, row in final_data.iterrows():\n",
    "  # Extract predicted labels\n",
    "  pred_conditions = row['Predicted Label_Conditions']\n",
    "  pred_study_title = row['Predicted Label_Study Title']\n",
    "  pred_outcomes = row['Predicted Label_Outcomes']\n",
    "  pred_brief_summary = row['Predicted Label_Brief Summary']\n",
    "  pred_others = row['Predicted Class']\n",
    "\n",
    "  # Collect all predictions into a list\n",
    "  all_predictions = [pred_conditions, pred_study_title, pred_outcomes, pred_brief_summary, pred_others]\n",
    "\n",
    "  # Apply majority voting: Predict 1 if majority are 1, else 0\n",
    "  final_prediction = 1 if sum(all_predictions) >= 3 else 0  # Majority threshold is 3/5\n",
    "  final_predictions.append(final_prediction)\n",
    "\n",
    "# Append the final predictions as a new column to \"final_data\"\n",
    "final_data[\"Final Prediction (Heuristic)\"] = final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = final_data[['NCT Number','Final Prediction_Max_Voting','Final Prediction (Heuristic)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results.to_csv(folder_path+'\\Results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
